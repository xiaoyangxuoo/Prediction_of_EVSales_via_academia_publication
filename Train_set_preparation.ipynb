{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata  = pd.read_csv('Xdata.csv')#publication data\n",
    "Ydata  = pd.read_csv('Ydata.csv')#EV_sales data\n",
    "Ydata['Years'] = Ydata['Years'].astype('int64')\n",
    "idx_of_invalid_years = Xdata[Xdata['Year'].isnull()].index\n",
    "Xdata.drop(idx_of_invalid_years,inplace=True)\n",
    "Xdata['Year'] = Xdata['Year'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Years</th>\n",
       "      <th>Total Citations per Year</th>\n",
       "      <th>Keyword_SEI</th>\n",
       "      <th>Keyword_cathode</th>\n",
       "      <th>Keyword_anode</th>\n",
       "      <th>Keyword_Efficiency</th>\n",
       "      <th>Keyword_electrode</th>\n",
       "      <th>Ci_5IPub</th>\n",
       "      <th>EV_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1997</td>\n",
       "      <td>3819.0</td>\n",
       "      <td>787.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1230.0</td>\n",
       "      <td>3564.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1998</td>\n",
       "      <td>11862.0</td>\n",
       "      <td>1641.0</td>\n",
       "      <td>870.0</td>\n",
       "      <td>1832.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2161.0</td>\n",
       "      <td>11036.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999</td>\n",
       "      <td>12632.0</td>\n",
       "      <td>3726.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3697.0</td>\n",
       "      <td>11735.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000</td>\n",
       "      <td>18320.0</td>\n",
       "      <td>1612.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>11235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10986.0</td>\n",
       "      <td>17070.0</td>\n",
       "      <td>9350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001</td>\n",
       "      <td>25058.0</td>\n",
       "      <td>7802.0</td>\n",
       "      <td>5536.0</td>\n",
       "      <td>6234.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>5795.0</td>\n",
       "      <td>22078.0</td>\n",
       "      <td>20282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2002</td>\n",
       "      <td>23792.0</td>\n",
       "      <td>6552.0</td>\n",
       "      <td>3572.0</td>\n",
       "      <td>4739.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7108.0</td>\n",
       "      <td>21639.0</td>\n",
       "      <td>36042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2003</td>\n",
       "      <td>24569.0</td>\n",
       "      <td>10066.0</td>\n",
       "      <td>2250.0</td>\n",
       "      <td>6302.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>4616.0</td>\n",
       "      <td>23795.0</td>\n",
       "      <td>47566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2004</td>\n",
       "      <td>27065.0</td>\n",
       "      <td>11537.0</td>\n",
       "      <td>4260.0</td>\n",
       "      <td>7379.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>8598.0</td>\n",
       "      <td>25157.0</td>\n",
       "      <td>84233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2005</td>\n",
       "      <td>31693.0</td>\n",
       "      <td>8870.0</td>\n",
       "      <td>3508.0</td>\n",
       "      <td>5533.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5693.0</td>\n",
       "      <td>29734.0</td>\n",
       "      <td>205876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006</td>\n",
       "      <td>42911.0</td>\n",
       "      <td>13445.0</td>\n",
       "      <td>5101.0</td>\n",
       "      <td>8163.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>13573.0</td>\n",
       "      <td>36222.0</td>\n",
       "      <td>251864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2007</td>\n",
       "      <td>37763.0</td>\n",
       "      <td>9248.0</td>\n",
       "      <td>4951.0</td>\n",
       "      <td>9283.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>11843.0</td>\n",
       "      <td>34395.0</td>\n",
       "      <td>351071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2008</td>\n",
       "      <td>55467.0</td>\n",
       "      <td>18696.0</td>\n",
       "      <td>7876.0</td>\n",
       "      <td>23874.0</td>\n",
       "      <td>746.0</td>\n",
       "      <td>13625.0</td>\n",
       "      <td>47382.0</td>\n",
       "      <td>315763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2009</td>\n",
       "      <td>22914.0</td>\n",
       "      <td>10302.0</td>\n",
       "      <td>2912.0</td>\n",
       "      <td>8758.0</td>\n",
       "      <td>2074.0</td>\n",
       "      <td>4507.0</td>\n",
       "      <td>19595.0</td>\n",
       "      <td>290273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2010</td>\n",
       "      <td>36535.0</td>\n",
       "      <td>14970.0</td>\n",
       "      <td>2688.0</td>\n",
       "      <td>20245.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>7465.0</td>\n",
       "      <td>32623.0</td>\n",
       "      <td>274648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2011</td>\n",
       "      <td>36645.0</td>\n",
       "      <td>12914.0</td>\n",
       "      <td>6110.0</td>\n",
       "      <td>12133.0</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>8072.0</td>\n",
       "      <td>30934.0</td>\n",
       "      <td>284264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2012</td>\n",
       "      <td>48879.0</td>\n",
       "      <td>17777.0</td>\n",
       "      <td>4432.0</td>\n",
       "      <td>22359.0</td>\n",
       "      <td>3139.0</td>\n",
       "      <td>15371.0</td>\n",
       "      <td>42813.0</td>\n",
       "      <td>487819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2013</td>\n",
       "      <td>46804.0</td>\n",
       "      <td>17964.0</td>\n",
       "      <td>8520.0</td>\n",
       "      <td>17380.0</td>\n",
       "      <td>2779.0</td>\n",
       "      <td>7112.0</td>\n",
       "      <td>42468.0</td>\n",
       "      <td>592637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2014</td>\n",
       "      <td>41621.0</td>\n",
       "      <td>15891.0</td>\n",
       "      <td>6025.0</td>\n",
       "      <td>21083.0</td>\n",
       "      <td>416.0</td>\n",
       "      <td>7361.0</td>\n",
       "      <td>38574.0</td>\n",
       "      <td>571054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2015</td>\n",
       "      <td>28485.0</td>\n",
       "      <td>13883.0</td>\n",
       "      <td>1322.0</td>\n",
       "      <td>9186.0</td>\n",
       "      <td>1802.0</td>\n",
       "      <td>6705.0</td>\n",
       "      <td>25135.0</td>\n",
       "      <td>498423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2016</td>\n",
       "      <td>26364.0</td>\n",
       "      <td>10504.0</td>\n",
       "      <td>2344.0</td>\n",
       "      <td>10425.0</td>\n",
       "      <td>796.0</td>\n",
       "      <td>4009.0</td>\n",
       "      <td>24045.0</td>\n",
       "      <td>506565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017</td>\n",
       "      <td>22815.0</td>\n",
       "      <td>11551.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>12521.0</td>\n",
       "      <td>1838.0</td>\n",
       "      <td>4021.0</td>\n",
       "      <td>19013.0</td>\n",
       "      <td>558449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2018</td>\n",
       "      <td>13577.0</td>\n",
       "      <td>7016.0</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>7083.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>2233.0</td>\n",
       "      <td>12852.0</td>\n",
       "      <td>704534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2019</td>\n",
       "      <td>2686.0</td>\n",
       "      <td>1481.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>1226.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>2580.0</td>\n",
       "      <td>727390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Years  Total Citations per Year  Keyword_SEI  Keyword_cathode  \\\n",
       "0    1997                    3819.0        787.0            183.0   \n",
       "1    1998                   11862.0       1641.0            870.0   \n",
       "2    1999                   12632.0       3726.0            469.0   \n",
       "3    2000                   18320.0       1612.0            309.0   \n",
       "4    2001                   25058.0       7802.0           5536.0   \n",
       "5    2002                   23792.0       6552.0           3572.0   \n",
       "6    2003                   24569.0      10066.0           2250.0   \n",
       "7    2004                   27065.0      11537.0           4260.0   \n",
       "8    2005                   31693.0       8870.0           3508.0   \n",
       "9    2006                   42911.0      13445.0           5101.0   \n",
       "10   2007                   37763.0       9248.0           4951.0   \n",
       "11   2008                   55467.0      18696.0           7876.0   \n",
       "12   2009                   22914.0      10302.0           2912.0   \n",
       "13   2010                   36535.0      14970.0           2688.0   \n",
       "14   2011                   36645.0      12914.0           6110.0   \n",
       "15   2012                   48879.0      17777.0           4432.0   \n",
       "16   2013                   46804.0      17964.0           8520.0   \n",
       "17   2014                   41621.0      15891.0           6025.0   \n",
       "18   2015                   28485.0      13883.0           1322.0   \n",
       "19   2016                   26364.0      10504.0           2344.0   \n",
       "20   2017                   22815.0      11551.0            998.0   \n",
       "21   2018                   13577.0       7016.0           1185.0   \n",
       "22   2019                    2686.0       1481.0            182.0   \n",
       "\n",
       "    Keyword_anode  Keyword_Efficiency  Keyword_electrode  Ci_5IPub  EV_Sales  \n",
       "0           635.0                 0.0             1230.0    3564.0         0  \n",
       "1          1832.0                 0.0             2161.0   11036.0         0  \n",
       "2          2120.0                49.0             3697.0   11735.0        17  \n",
       "3         11235.0                 0.0            10986.0   17070.0      9350  \n",
       "4          6234.0               176.0             5795.0   22078.0     20282  \n",
       "5          4739.0                 7.0             7108.0   21639.0     36042  \n",
       "6          6302.0               323.0             4616.0   23795.0     47566  \n",
       "7          7379.0               601.0             8598.0   25157.0     84233  \n",
       "8          5533.0                 0.0             5693.0   29734.0    205876  \n",
       "9          8163.0               247.0            13573.0   36222.0    251864  \n",
       "10         9283.0               756.0            11843.0   34395.0    351071  \n",
       "11        23874.0               746.0            13625.0   47382.0    315763  \n",
       "12         8758.0              2074.0             4507.0   19595.0    290273  \n",
       "13        20245.0               156.0             7465.0   32623.0    274648  \n",
       "14        12133.0              1997.0             8072.0   30934.0    284264  \n",
       "15        22359.0              3139.0            15371.0   42813.0    487819  \n",
       "16        17380.0              2779.0             7112.0   42468.0    592637  \n",
       "17        21083.0               416.0             7361.0   38574.0    571054  \n",
       "18         9186.0              1802.0             6705.0   25135.0    498423  \n",
       "19        10425.0               796.0             4009.0   24045.0    506565  \n",
       "20        12521.0              1838.0             4021.0   19013.0    558449  \n",
       "21         7083.0               523.0             2233.0   12852.0    704534  \n",
       "22         1226.0                76.0              249.0    2580.0    727390  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Xdata.head(20)\n",
    "Ydata.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xdata['Feed'] =  Xdata['Title'] + '. '+ Xdata['Abstract']\n",
    "# for i in range(len(Xdata)):    \n",
    "#       Xdata.iloc[i,17] = Xdata.iloc[i,17].ljust(Xdata['Feed'].str.len().max(),'a')#Padding Title\n",
    "# Word_cloud = []\n",
    "# for i in Xdata['Feed']:\n",
    "#      for j in i:\n",
    "#          Word_cloud.append(ord(j))\n",
    "substr = {'1997': ' ','1998': ' ','1999': ' ','2000': ' ','2001': ' ','2002': ' ','2003': ' ','2004': ' ','2005': ' ',\\\n",
    "          '2006': ' ','2007': ' ','2008': ' ', '2009': ' ','2010': ' ','2011': ' ','2012': ' ','2013': ' ','2014': ' ',\\\n",
    "         '2015': ' ','2016': ' ','2017': ' ','2018': ' ','2019': ' '}\n",
    "for i in range(Xdata['Year'].min(),Xdata['Year'].max()+1):\n",
    "    for j in Xdata[Xdata['Year'] == i]['Feed']:\n",
    "        try:\n",
    "            substr[str(i)] += '. '+ j\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "###########    padding the string in each column to make them same length\n",
    "max_l = len(substr['1997'])\n",
    "for key in substr.keys():\n",
    "    if max_l < len(substr[key]):\n",
    "        max_l = len(substr[key])   \n",
    "for key in substr.keys():\n",
    "    substr[key] = substr[key].ljust(max_l,'a')    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnum_copy = {'1997': [],'1998': [],'1999': [],'2000': [],'2001': [],'2002': [],'2003': [],'2004': [],'2005': [],\\\n",
    "          '2006': [],'2007': [],'2008': [], '2009': [],'2010': [],'2011': [],'2012': [],'2013': [],'2014': [],\\\n",
    "         '2015': [],'2016': [],'2017': [],'2018': [],'2019': []}\n",
    "for key in substr.keys():\n",
    "    for string in substr[key]:\n",
    "        subnum_copy[key].append(ord(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103882"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subnum_copy['2002'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "#collecting all x and y\n",
    "all_x = []\n",
    "all_y = []\n",
    "for key in substr.keys(): \n",
    "    try:\n",
    "        subnum_copy[key].append(Ydata[Ydata['Years']==int(key)]['Total Citations per Year'].values[0])\n",
    "    except:\n",
    "        pass\n",
    "    temp_x = subnum_copy[key]\n",
    "    temp_y = Ydata[Ydata['Years']==int(key)]['EV_Sales'].values[0]\n",
    "    all_x.append(temp_x)\n",
    "    #train_y2 = np.append(train_y2, np.asarray(train_y).astype('float64').reshape(-1, 1))\n",
    "    all_y.append(temp_y)\n",
    "print(len(all_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727390"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_y[-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the x_input vs. y_input up to year of 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# def vectorize_sequences(sequences, dimension=10000):\n",
    "#     # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "#     results = np.zeros((len(sequences), dimension))\n",
    "#     for i, sequence in enumerate(sequences):\n",
    "#         results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
    "#     return results\n",
    "\n",
    "# # Our vectorized training data\n",
    "# x_train = vectorize_sequences(train_data)\n",
    "# # Our vectorized test data\n",
    "# x_test = vectorize_sequences(test_data)\n",
    "#train the x input vs. y input \n",
    "Xtrainwhole = np.asarray(all_x[:-1]).astype('float64')#.reshape(23, 1)\n",
    "Ytrainwhole = np.asarray(all_y[:-1]).astype('float64').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of Training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler1 = MinMaxScaler()#this is a must\n",
    "train_x = scaler1.fit_transform(Xtrainwhole)\n",
    "scaler2 = MinMaxScaler()\n",
    "train_y = scaler2.fit_transform(Ytrainwhole)\n",
    "def build_model(neurons=32):\n",
    "    # Because we will need to instantiate\n",
    "    # the same model multiple times,\n",
    "    # we use a function to construct it.\n",
    "    \n",
    "    model = models.Sequential() \n",
    "    \n",
    "    model.add(layers.Dense(128, activation='relu',\n",
    "                           input_shape=(train_x.shape[1],)))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    #model.add(layers.Dense(32, activation='relu'))\n",
    "    model.add(layers.Dense(6, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross validation of the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "###number of folds\n",
    "k = 10\n",
    "num_val_samples = len(train_x) // k\n",
    "num_epochs = 50\n",
    "all_scores = []\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_x[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_x = np.concatenate(\n",
    "        [train_x[:i * num_val_samples],\n",
    "         train_x[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_y = np.concatenate(\n",
    "        [train_y[:i * num_val_samples],\n",
    "         train_y[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    model.fit(partial_train_x, partial_train_y,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    # Evaluate the model on the validation data\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Some memory clean-up\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the model to some extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "k = 10\n",
    "for i in range(k):\n",
    "    print('processing fold #', i)\n",
    "    # Prepare the validation data: data from partition # k\n",
    "    val_data = train_x[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_y[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "    # Prepare the training data: data from all other partitions\n",
    "    partial_train_x = np.concatenate(\n",
    "        [train_x[:i * num_val_samples],\n",
    "         train_x[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_y = np.concatenate(\n",
    "        [train_y[:i * num_val_samples],\n",
    "         train_y[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "\n",
    "    # Build the Keras model (already compiled)\n",
    "    model = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "    history = model.fit(partial_train_x, partial_train_y,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    mae_history = history.history['val_mae']\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the modelling error(Mean absolute error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs') \n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point * (1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "smooth_mae_history = smooth_curve(average_mae_history[10:])\n",
    "\n",
    "plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "model_v3 = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "model_v3.fit(train_x, train_y,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test  data\n",
    "predictions_2019 = model_v1.predict(np.asarray([all_x[-1]]).astype('float64'))\n",
    "add_neuron = 32\n",
    "loop_indicator = 1\n",
    "while predictions_2019 < all_y[-1]-5000:\n",
    "    add_neuron *= 2\n",
    "    num_epochs += 50\n",
    "    model_v1 = build_model(add_neuron)\n",
    "    print('Processing loop: ', str(loop_indicator))\n",
    "    model_v1.fit(train_x, train_y,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test  data\n",
    "    predictions_2019 = model_v1.predict(np.asarray([all_x[-1]]).astype('float64'))\n",
    "    loop_indicator += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "model_v5 = build_model()\n",
    "    # Train the model (in silent mode, verbose=0)\n",
    "model_v5.fit(train_x, train_y,\n",
    "              epochs=num_epochs, batch_size=1, verbose=0)\n",
    "\n",
    "    # Evaluate the model on the test  data\n",
    "predictions_2019 = model_v5.predict(np.asarray([all_x[-1]]).astype('float64'))\n",
    "print(scaler2.inverse_transform(predictions_2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.predict(np.asarray([all_x[-1]]).astype('float64'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               13297152  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 6)                 102       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 13,299,325\n",
      "Trainable params: 13,299,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
